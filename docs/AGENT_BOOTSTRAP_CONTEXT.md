# ZenGlow Beast3 Agent Bootstrap Context (Expanded v1.1)

Purpose: Provide a compact, high–signal knowledge pack for an on‑host AI / Copilot agent to operate the local LLM (Ollama + gemma2b), interact with TimescaleDB + pgvector tables, and perform RAG style retrieval / generation.

Keep this file lean; update when schemas or service endpoints change. Do NOT paste secrets here.

---
## 1. Runtime Components (Beast3)
| Component | Purpose | Key Endpoint / Path |
|-----------|---------|----------------------|
| Ollama Service | Local inference (gemma:2b + gemma2b-q4) | http://127.0.0.1:11434 |
| TimescaleDB (Postgres + extensions) | Time‑series (metrics) + vector search (embeddings) | Host: localhost:5432 DB: zenglow |
| FastAPI (dev-indexer_1/app) | API surface (generation, future embedding ingest) | (Start via uvicorn main:app) |

Expected extensions in DB: `timescaledb`, `vector`.

---
## 2. Environment Variables (.env exemplar)
```
DATABASE_URL=postgresql://zenglow:examplepassword@localhost:5432/zenglow
OLLAMA_HOST=127.0.0.1
OLLAMA_PORT=11434
OLLAMA_MODEL=gemma:2b
PG_EMBED_DIM=768
```
If quantized variant desired: `OLLAMA_MODEL=gemma2b-q4`.

---
## 3. File / Directory Pointers
```
dev-indexer_1/
  scripts/
    ollama_steps.sh              # Granular step executor (STEP=1..5)
    remote_ollama_bootstrap.sh   # One-shot home-dir bootstrap
  app/
    main.py                      # FastAPI entry (uses OLLAMA_* env vars)
  docs/
    AGENT_BOOTSTRAP_CONTEXT.md   # (this file)
```

---
## 4. Ollama Operations
Health / tags:
```
GET /api/tags        -> list models
POST /api/generate   JSON: {"model":"gemma:2b","prompt":"<text>"}
```
Shell examples:
```
curl -s http://127.0.0.1:11434/api/tags | jq '.'
curl -s http://127.0.0.1:11434/api/generate \
  -d '{"model":"gemma:2b","prompt":"ping"}' | sed '5q'
```

---
## 5. Database Schema (Minimal RAG Core)
Embeddings table (example – confirm actual live schema):
```sql
CREATE TABLE IF NOT EXISTS doc_embeddings (
  id BIGSERIAL PRIMARY KEY,
  source TEXT,
  chunk TEXT NOT NULL,
  embedding vector(768),
  created_at TIMESTAMPTZ DEFAULT now()
);
```
Time‑series metrics (hypertable):
```sql
CREATE TABLE IF NOT EXISTS device_metrics (
  time TIMESTAMPTZ NOT NULL,
  device_id TEXT NOT NULL,
  metric TEXT NOT NULL,
  value DOUBLE PRECISION,
  PRIMARY KEY(time, device_id, metric)
);
SELECT create_hypertable('device_metrics','time', if_not_exists=>TRUE);
```
Vector similarity query pattern:
```sql
SELECT id, source, left(chunk,120) snippet
FROM doc_embeddings
ORDER BY embedding <-> $1::vector
LIMIT 5;
```

---
## 6. Retrieval + Generation Flow (Target Pattern)
1. Receive user query Q.
2. Generate embedding e(Q) (NOTE: currently placeholder; integrate real embedding model soon).
3. Retrieve top-k chunks via `ORDER BY embedding <-> e(Q)`.
4. Build prompt:
   ```
   You are ZenGlow Assistant. Use context to answer.
   Context:\n<chunk_1>\n---\n<chunk_2> ...
   Question: <Q>
   Answer:
   ```
5. Call Ollama `/api/generate` with `model=OLLAMA_MODEL`.
6. Stream / collect response; optionally store Q/A + metadata.

---
## 7. Embedding Generation (Interim Strategy)
Current: gemma:2b is a chat model; embeddings should be generated by a dedicated embedding model (e.g. bge-m3, bge-base-en, all-mpnet-base-v2). Until integrated, avoid storing low‑quality vectors produced via prompt hacks. Agent SHOULD mark any placeholder vectors with source tag `placeholder` so they can be reprocessed.

Planned integration stub (Python pseudocode):
```python
def get_embedding(text: str) -> list[float]:
    # Replace with sentence-transformers or local embedding model.
    raise NotImplementedError
```

---
## 8. FastAPI `main.py` (Key Points)
Reads `OLLAMA_HOST`, `OLLAMA_PORT`, constructs generate URL:
`http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/generate`
Add new endpoints for:
```
GET /health/ollama  -> 200 if /api/tags reachable
GET /health/db      -> 200 if simple SELECT 1
POST /rag/query     -> {query: str, top_k?: int} -> retrieval + LLM answer
```

---
## 9. Operational Commands (Agent Cheat Sheet)
```
# Ollama status
systemctl is-active ollama || systemctl status --no-pager ollama

# List models
ollama list

# Tail service log
journalctl -u ollama -n 100 --no-pager

# DB connectivity check
psql "$DATABASE_URL" -c 'SELECT 1;' 2>&1 | grep -q "1 row" && echo DB_OK || echo DB_FAIL

# Top 3 retrieved snippets for a dummy zero vector (diagnostic)
psql "$DATABASE_URL" -c "SELECT id, left(chunk,60) FROM doc_embeddings ORDER BY embedding <-> '[0,0]'::vector LIMIT 3;"
```

---
## 10. Safety / Boundaries
Do NOT store raw secrets or user PII in logs. Scrub before persistence.
Avoid embedding extremely large raw files; chunk text (target 512‑1024 tokens) first.
Ensure no prompts include .env contents or private keys.

---
## 11. Next Improvements (Backlog)
- Integrate proper embedding model + batch ingestion script.
- Add approximate index (pgvector IVF / HNSW) after enough rows (>10k).
- Move model dir to ZFS (/tank/ollama/models) once pool available.
- Add caching layer for recent RAG answers.
- Add metrics: latency, tokens, retrieval hit counts.

---
## 12. Minimal Agent Self‑Test Script (Optional)
```bash
python - <<'PY'
import os, requests, json
host=os.getenv('OLLAMA_HOST','127.0.0.1'); port=os.getenv('OLLAMA_PORT','11434')
r=requests.post(f'http://{host}:{port}/api/generate', json={'model':os.getenv('OLLAMA_MODEL','gemma:2b'),'prompt':'ping'}, stream=True)
acc='';
for line in r.iter_lines():
    if not line: continue
    j=json.loads(line); acc += j.get('response','');
    if len(acc)>20: break
print('RESP:',acc.strip())
PY
```

---
## 13. Contact / Update Protocol
When schema or env var changes: update this file section 2 or 5 and increment a CHANGE NOTE at bottom.

---
## 14. Detailed API Contract (Planned / Recommended)

### 14.1 Generation Endpoint (Existing / Minimal)
`POST /generate`
Request JSON:
```json
{
  "prompt": "<user prompt>",
  "model": "optional override (default env OLLAMA_MODEL)",
  "stream": true
}
```
Response (stream or single JSON): aggregator should capture `response` tokens.

### 14.2 Health Endpoints
| Path | Logic | 200 Criteria |
|------|-------|--------------|
| GET /health/ollama | GET /api/tags on Ollama | HTTP 200 and JSON parse |
| GET /health/db | `SELECT 1` on Postgres | Returns 1 row |
| GET /health/vector | `SELECT avg(embedding[1]) FROM doc_embeddings LIMIT 1` | Query ok (table may be empty) |

### 14.3 RAG Query Endpoint
`POST /rag/query`
```json
{
  "query": "How does sleep affect mood?",
  "top_k": 5,
  "include_chunks": true,
  "model": "gemma:2b"
}
```
Response:
```json
{
  "answer": "<LLM answer>",
  "chunks": [ {"id":123, "score":0.12, "source":"docX", "chunk":"..."} ],
  "token_usage": {"prompt": 512, "completion": 128}
}
```

### 14.4 Embedding Insert Endpoint (Future)
`POST /embeddings/ingest`
```json
{
  "documents": [
    {"id":"optional-external-id","text":"raw text ...","source":"<origin>"},
    {"text":"..."}
  ],
  "batch_tag":"import_2025_08_25"
}
```
Response contains counts + rejected docs list.

---
## 15. Embedding Pipeline (Target Implementation Notes)
1. Preprocess: split documents into ~800 token chunks (overlap 80 tokens). Maintain `source` + `ordinal`.
2. Deduplicate: avoid inserting identical `chunk` (hash check SHA256).
3. Embed: call dedicated embedding model (NOT gemma) – placeholder now.
4. Normalize: L2 normalize vector if using cosine distance; if using `<->` (Euclidean / inner product) adjust index accordingly.
5. Insert: single transaction for batch of N (e.g. 64). Use `executemany` or COPY for speed.
6. Refresh any approximate index (if IVF/HNSW later) after large batch.

Pseudo-code:
```python
def ingest(chunks: list[str], meta_sources: list[str]):
    vecs = model.embed(chunks)  # -> List[List[float]] len=dim
    with conn, conn.cursor() as cur:
        cur.executemany(
            "INSERT INTO doc_embeddings (source, chunk, embedding) VALUES (%s,%s,%s)",
            list(zip(meta_sources, chunks, vecs))
        )
```

---
## 16. pgvector Performance & Indexing Strategy
| Stage | Threshold | Action |
|-------|-----------|--------|
| Cold start | < 1k rows | Sequential scan fine |
| Growth | 1k–10k | Consider partial IVF/HNSW test environment |
| Mature | > 10k | Create index; add maintenance window |

Example IVF index after enabling pgvector >=0.5:
```sql
CREATE INDEX IF NOT EXISTS doc_embeddings_embedding_ivf
ON doc_embeddings
USING ivfflat (embedding vector_l2_ops)
WITH (lists=100);
ANALYZE doc_embeddings;
```
HNSW (if extension build supports):
```sql
CREATE INDEX IF NOT EXISTS doc_embeddings_embedding_hnsw
ON doc_embeddings
USING hnsw (embedding vector_l2_ops)
WITH (m=16, ef_construction=64);
```
Run `EXPLAIN ANALYZE` periodically to validate planner usage.

Vacuum / Analyze cadence (cron / systemd):
```
psql $DATABASE_URL -c "VACUUM (ANALYZE) doc_embeddings;"
```

---
## 17. Observability Plan
Metrics to gather (exporter or simple log counters):
| Metric | Description |
|--------|-------------|
| rag_retrieval_ms | Time to run similarity query |
| rag_chunks_returned | Count of chunks in response |
| ollama_latency_ms | LLM generation wall time |
| tokens_prompt / tokens_completion | Token counts (estimate) |
| embedding_batch_ms | Time to embed batch |
| db_conn_pool_in_use | Current active DB connections |

Simple timing decorator (Python snippet):
```python
import time, logging
def timed(name):
    def wrap(fn):
        def inner(*a, **kw):
            t0=time.time();
            try: return fn(*a, **kw)
            finally:
                logging.info("metric=%s ms=%.1f" % (name,(time.time()-t0)*1000))
        return inner
    return wrap
```

---
## 18. Failure Modes & Mitigations
| Failure | Symptom | Mitigation |
|---------|---------|------------|
| Ollama down | 5xx / connection refused | systemctl restart ollama; check journalctl |
| Model missing | /api/generate error | `ollama pull gemma:2b` again |
| Vector extension missing | SQL error: type "vector" does not exist | `CREATE EXTENSION vector;` |
| Slow retrieval | High latency on query | Add IVF/HNSW index, increase RAM, analyze table |
| Memory bloat | OOM / kill | Limit concurrency, `OLLAMA_MAX_LOADED_MODELS=1` |
| Poor answers | Irrelevant context | Improve chunking, real embedding model, filtering |

---
## 19. Security Considerations
| Aspect | Guidance |
|--------|----------|
| Network | Keep Ollama bound to localhost unless auth proxy added |
| Secrets | Use .env (not committed) for DB creds; rotate password periodically |
| Prompt Injection | Strip system prompts from retrieved chunks; optionally apply regex sanitization |
| Data Provenance | Track `source` & possibly `batch_tag` columns for retraction |

Add `ALTER TABLE doc_embeddings ADD COLUMN IF NOT EXISTS batch_tag TEXT;` when provenance needed.

---
## 20. Migration & Schema Evolution
Use idempotent SQL migrations; naming convention: `YYYYMMDDHHMM_description.sql`.
Maintain a table:
```sql
CREATE TABLE IF NOT EXISTS schema_migrations (
  id SERIAL PRIMARY KEY,
  filename TEXT UNIQUE,
  applied_at TIMESTAMPTZ DEFAULT now()
);
```
Apply script:
```bash
for f in supabase/migrations/*.sql; do
  psql $DATABASE_URL -v ON_ERROR_STOP=1 -c "INSERT INTO schema_migrations(filename) VALUES ('$(basename $f)') ON CONFLICT DO NOTHING;" || exit 1
done
```

---
## 21. Roadmap Snapshot
Short term (Week 1): Health endpoints, real embedding model, ingestion script.
Mid term (Week 2–3): Index strategy rollout, metrics collection, model evaluation harness.
Long term: Multi-model orchestration (embedding + generative separation), autoscaling, ZFS migration for model storage.

---
## 22. Glossary
| Term | Definition |
|------|------------|
| RAG | Retrieval Augmented Generation |
| IVF | Inverted File index (partitioned vector index) |
| HNSW | Hierarchical Navigable Small World graph index for approximate NN |

---
CHANGE NOTE: v1.1 expanded with API contracts, performance, observability, security, roadmap.
