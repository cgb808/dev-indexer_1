model_name: microsoft/Phi-3-mini-4k-instruct
revision: main
trust_remote_code: true
bf16: true

# Dataset / IO
dataset_path: data/general/jeeves_general_dataset.jsonl
output_dir: models/ollama_general_phi3
log_dir: fine_tuning/training/logs

# Training Hyperparameters
num_epochs: 2
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_ratio: 0.05
max_seq_length: 2048
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
max_grad_norm: 1.0
lr_scheduler_type: cosine

# LoRA Config
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  bias: none
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]
  task_type: CAUSAL_LM

save_strategy: epoch
logging_steps: 50
evaluation_disabled: true
