#!/usr/bin/env python3
"""Hybrid (Methodology + Mathematics) Training Simulator / Driver

Purpose:
  Provide a reproducible, parameterized scaffold to (a) audit a hybrid dataset
  of pure methodology and subject-integrated (math) examples, and (b) simulate
  phased curricular fine-tuning interactions against an Ollama-served base
  model (default: phi3:mini). This DOES NOT actually fine-tune the base model;
  it exercises the model with structured prompts and captures responses for
  later comparative analysis / few-shot distillation.

Key Features vs initial draft:
  - CLI (argparse) for dataset path, model, phase sizes, temperature, etc.
  - Deterministic shuffling (seed) & stratified sampling per phase.
  - Automatic phase size downscaling if dataset < requested.
  - Retry + backoff on Ollama generate failures (network / transient).
  - Optional streaming mode (captures partial tokens incrementally).
  - Dry-run mode skips model calls for fast structural validation.
  - Structured JSON log (phases, latency stats, error counts).
  - Graceful interrupt handling (Ctrl+C) with partial log flush.
  - Simple health check of Ollama endpoint before first generate.
  - Output directory timestamped; symlink "latest" for convenience.

Future Enhancements (not implemented yet):
  - True fine-tune flow (requires model export + external trainer).
  - Metrics export (Prometheus / parquet) & evaluation harness.
  - Support additional subject domains beyond mathematics.

Dataset Requirements:
  Each JSONL line expected fields:
    instruction (str)
    output (str)
    example_type in {"pure_methodology", "mathematics_with_methodology"}
    methodology_focus (optional str for integrated examples)

Author: Automated augmentation generated by AI assistant.
License: Inherits repository license.
"""

from __future__ import annotations

import argparse
import json
import os
import random
import signal
import sys
import time
from collections import Counter, defaultdict
from dataclasses import dataclass, asdict
from datetime import datetime, UTC
from typing import Any, Dict, Iterable, List, Optional, Tuple

import requests


# ----------------------------- Data Classes -----------------------------

@dataclass
class PhaseConfig:
    name: str
    focus: str
    description: str
    examples: int

@dataclass
class PhaseStats:
    phase: str
    requested_examples: int
    used_examples: int
    pure_methodology: int
    math_with_methodology: int
    avg_latency_ms: float
    max_latency_ms: float
    min_latency_ms: float
    errors: int


# ----------------------------- Utility Functions -----------------------------

def load_jsonl(path: str) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        raise FileNotFoundError(f"Dataset not found: {path}")
    examples: List[Dict[str, Any]] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                examples.append(json.loads(line))
            except json.JSONDecodeError as e:
                raise ValueError(f"Malformed JSONL line: {e}: {line[:120]}") from e
    return examples


def health_check(ollama_url: str, timeout: float = 5.0) -> bool:
    try:
        r = requests.get(f"{ollama_url}/api/tags", timeout=timeout)
        return r.status_code == 200
    except Exception:
        return False


def stratified_sample(examples: List[Dict[str, Any]], k: int, rng: random.Random) -> List[Dict[str, Any]]:
    if k >= len(examples):
        return examples.copy()
    # Simple random sample; could extend to enforce ratio maintenance per type.
    return rng.sample(examples, k)


def chunk(iterable: Iterable[Any], size: int) -> Iterable[List[Any]]:
    batch: List[Any] = []
    for item in iterable:
        batch.append(item)
        if len(batch) >= size:
            yield batch
            batch = []
    if batch:
        yield batch


# ----------------------------- Core Trainer -----------------------------

class HybridTrainer:
    def __init__(
        self,
        dataset_path: str,
        model: str,
        ollama_url: str,
        seed: int = 42,
        temperature: float = 0.1,
        top_p: float = 0.9,
        max_tokens: int = 200,
        retries: int = 3,
        retry_backoff: float = 1.5,
        stream: bool = False,
        dry_run: bool = False,
    ) -> None:
        self.dataset_path = dataset_path
        self.model = model
        self.ollama_url = ollama_url.rstrip("/")
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        self.retries = retries
        self.retry_backoff = retry_backoff
        self.stream = stream
        self.dry_run = dry_run
        self.rng = random.Random(seed)
        self._stop = False
        signal.signal(signal.SIGINT, self._handle_interrupt)
        signal.signal(signal.SIGTERM, self._handle_interrupt)

        self.examples: List[Dict[str, Any]] = []
        self.output_dir: Optional[str] = None
        self.logs: Dict[str, Any] = {}

    # --------------------- Public API ---------------------
    def run(self, phases: List[PhaseConfig]) -> str:
        self._prepare_output_dir()
        self.examples = load_jsonl(self.dataset_path)
        self._shuffle_examples()
        self._analyze_dataset()
        self._init_log(phases)

        if not self.dry_run and not health_check(self.ollama_url):
            print("[WARN] Ollama health check failed; proceeding (responses may error).", file=sys.stderr)

        # Phase execution
        remaining = self.examples.copy()
        for idx, phase in enumerate(phases, 1):
            if self._stop:
                print("[INFO] Interrupted before phase execution complete.")
                break
            print(f"\n=== Phase {idx}: {phase.name} ===")
            used = min(phase.examples, len(remaining))
            phase_slice = stratified_sample(remaining, used, self.rng)
            # Optionally remove used examples to avoid reuse
            id_set = {id(ex) for ex in phase_slice}
            remaining = [ex for ex in remaining if id(ex) not in id_set]
            phase_stats = self._process_phase(idx, phase, phase_slice)
            self.logs["phases"][f"phase_{idx}"] = asdict(phase_stats)
            self._flush_log()
            if not remaining:
                print("[INFO] Dataset exhausted before all phases consumed.")
                break

        self.logs["end_time"] = datetime.now(UTC).isoformat()
        self.logs["status"] = "completed" if not self._stop else "interrupted"
        self._flush_log(final=True)
        print(f"\n[RESULT] Log directory: {self.output_dir}")
        return self.output_dir or ""

    # --------------------- Internal Helpers ---------------------
    def _prepare_output_dir(self) -> None:
        ts = datetime.now(UTC).strftime("%Y%m%d_%H%M%S")
        base_dir = os.path.join("models", "hybrid_phi3")
        os.makedirs(base_dir, exist_ok=True)
        out_dir = os.path.join(base_dir, ts)
        os.makedirs(out_dir, exist_ok=True)
        latest_link = os.path.join(base_dir, "latest")
        try:
            if os.path.islink(latest_link) or os.path.exists(latest_link):
                os.unlink(latest_link)
            os.symlink(os.path.abspath(out_dir), latest_link)
        except OSError:
            pass
        self.output_dir = out_dir

    def _shuffle_examples(self) -> None:
        self.rng.shuffle(self.examples)

    def _analyze_dataset(self) -> None:
        counts = Counter(ex.get("example_type", "unknown") for ex in self.examples)
        print("[DATASET] Loaded {} examples".format(len(self.examples)))
        for k, v in sorted(counts.items()):
            print(f"  - {k}: {v}")
        self.logs["dataset_composition"] = dict(counts)

    def _init_log(self, phases: List[PhaseConfig]) -> None:
        self.logs.update(
            {
                "model": self.model,
                "ollama_url": self.ollama_url,
                "training_type": "hybrid_methodology_mathematics_simulation",
                "temperature": self.temperature,
                "top_p": self.top_p,
                "max_tokens": self.max_tokens,
                "retries": self.retries,
                "stream": self.stream,
                "dry_run": self.dry_run,
                "phases_config": [asdict(p) for p in phases],
                "start_time": datetime.now(UTC).isoformat(),
                "phases": {},
            }
        )

    def _process_phase(self, idx: int, phase: PhaseConfig, examples: List[Dict[str, Any]]) -> PhaseStats:
        latencies: List[float] = []
        errors = 0
        sample_previews: List[Dict[str, str]] = []

        preview_count = min(3, len(examples))
        for i, ex in enumerate(examples):
            if self._stop:
                break
            start = time.perf_counter()
            response_text = self._query_model(ex) if not self.dry_run else "[DRY_RUN_RESPONSE]"
            latency_ms = (time.perf_counter() - start) * 1000.0
            latencies.append(latency_ms)
            if isinstance(response_text, str) and response_text.startswith("[ERROR]"):
                errors += 1
            if i < preview_count:
                fmt_prompt, fmt_expected = self._format_example(ex)
                sample_previews.append(
                    {
                        "prompt": fmt_prompt[:180],
                        "expected": fmt_expected[:180],
                        "model_response": (response_text or "")[:180],
                        "example_type": ex.get("example_type", "unknown"),
                        "latency_ms": round(latency_ms, 2),
                    }
                )

        # Write per-phase preview file
        phase_preview_path = os.path.join(self.output_dir or ".", f"phase_{idx}_samples.json")
        with open(phase_preview_path, "w", encoding="utf-8") as f:
            json.dump(sample_previews, f, indent=2)
        print(f"[PHASE {idx}] Wrote sample preview: {phase_preview_path}")

        type_counts = Counter(ex.get("example_type", "unknown") for ex in examples)
        return PhaseStats(
            phase=phase.name,
            requested_examples=phase.examples,
            used_examples=len(examples),
            pure_methodology=type_counts.get("pure_methodology", 0),
            math_with_methodology=type_counts.get("mathematics_with_methodology", 0),
            avg_latency_ms=sum(latencies) / len(latencies) if latencies else 0.0,
            max_latency_ms=max(latencies) if latencies else 0.0,
            min_latency_ms=min(latencies) if latencies else 0.0,
            errors=errors,
        )

    def _format_example(self, example: Dict[str, Any]) -> Tuple[str, str]:
        instr = example.get("instruction", "")
        out = example.get("output", "")
        etype = example.get("example_type", "unknown")
        if etype == "pure_methodology":
            prompt = f"[HYBRID_TRAINING_MODE][PURE_METHODOLOGY] {instr}".strip()
            expected = f"[METHODOLOGY_RESPONSE] {out}".strip()
        else:
            focus = example.get("methodology_focus", "General")
            prompt = f"[HYBRID_TRAINING_MODE][MATH_WITH_METHODOLOGY][{focus}] {instr}".strip()
            expected = f"[INTEGRATED_RESPONSE] {out}".strip()
        return prompt, expected

    def _query_model(self, example: Dict[str, Any]) -> str:
        prompt, _ = self._format_example(example)
        hybrid_context = (
            "[HYBRID_MODE] Integrate pure teaching methodology with subject-specific content. "
            "Apply pedagogical techniques while maintaining mathematical accuracy."
        )
        full_prompt = f"{hybrid_context}\n\n{prompt}"[:8000]  # safety truncation

        payload = {
            "model": self.model,
            "prompt": full_prompt,
            "stream": self.stream,
            "options": {
                "temperature": self.temperature,
                "top_p": self.top_p,
                "max_tokens": self.max_tokens,
            },
        }

        for attempt in range(1, self.retries + 1):
            try:
                if self.stream:
                    with requests.post(
                        f"{self.ollama_url}/api/generate", json=payload, timeout=60, stream=True
                    ) as r:
                        if r.status_code != 200:
                            raise RuntimeError(f"HTTP {r.status_code}")
                        out_parts: List[str] = []
                        for line in r.iter_lines():
                            if not line:
                                continue
                            try:
                                obj = json.loads(line.decode("utf-8"))
                                token = obj.get("response")
                                if token:
                                    out_parts.append(token)
                                if obj.get("done"):
                                    break
                            except json.JSONDecodeError:
                                continue
                        return "".join(out_parts).strip() or "[EMPTY_RESPONSE]"
                else:
                    r = requests.post(
                        f"{self.ollama_url}/api/generate", json=payload, timeout=60
                    )
                    if r.status_code != 200:
                        raise RuntimeError(f"HTTP {r.status_code}")
                    data = r.json()
                    return data.get("response", "[NO_RESPONSE]").strip()
            except Exception as e:  # noqa: BLE001
                if attempt == self.retries:
                    return f"[ERROR] {e}"
                sleep_for = self.retry_backoff ** (attempt - 1)
                time.sleep(sleep_for)
        return "[UNREACHABLE]"  # defensive

    def _flush_log(self, final: bool = False) -> None:
        if not self.output_dir:
            return
        path = os.path.join(self.output_dir, "hybrid_training_log.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.logs, f, indent=2)
        if final:
            meta_path = os.path.join(self.output_dir, "model_metadata.json")
            meta = self._build_metadata()
            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(meta, f, indent=2)

    def _build_metadata(self) -> Dict[str, Any]:
        return {
            "model_name": f"Hybrid Methodology + Mathematics ({self.model})",
            "base_model": self.model,
            "simulation": True,
            "specialization": "hybrid_methodology_mathematics",
            "dataset_path": self.dataset_path,
            "total_examples": len(self.examples),
            "dataset_composition": self.logs.get("dataset_composition", {}),
            "training_date": datetime.now(UTC).isoformat(),
            "focus": "Integration of pure pedagogy with subject (mathematics) content",
            "capabilities": [
                "Pure tutoring methodology application",
                "Mathematics-specific teaching technique augmentation",
                "Integrated pedagogical reasoning",
                "Seamless methodology-content synthesis",
            ],
            "notes": "Simulation only; does not alter base model weights.",
        }

    def _handle_interrupt(self, signum, frame):  # noqa: D401, ANN001, D417
        print(f"\n[INTERRUPT] Signal {signum} received; finishing current item then exiting...", file=sys.stderr)
        self._stop = True


# ----------------------------- CLI Parsing -----------------------------


def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Hybrid methodology + mathematics training simulator")
    p.add_argument("--dataset", default="data/hybrid/hybrid_methodology_math_dataset.jsonl", help="Path to hybrid dataset JSONL")
    p.add_argument("--model", default="phi3:mini", help="Ollama model name/tag")
    p.add_argument("--ollama-url", default="http://localhost:11435", help="Base URL for Ollama service")
    p.add_argument("--phase-sizes", default="500,300,200", help="Comma separated example counts for phases")
    p.add_argument("--seed", type=int, default=42, help="Deterministic RNG seed")
    p.add_argument("--temperature", type=float, default=0.1)
    p.add_argument("--top-p", type=float, default=0.9)
    p.add_argument("--max-tokens", type=int, default=200)
    p.add_argument("--retries", type=int, default=3)
    p.add_argument("--retry-backoff", type=float, default=1.5, help="Exponential backoff factor")
    p.add_argument("--stream", action="store_true", help="Enable streaming generation")
    p.add_argument("--dry-run", action="store_true", help="Skip model calls")
    return p.parse_args(argv)


def build_phase_configs(sizes: List[int]) -> List[PhaseConfig]:
    templates = [
        ("Foundation Integration", "Balance pure methodology with subject application", "Build integrated base"),
        ("Advanced Synthesis", "Deepen methodology-content interplay", "Refine synthesis"),
        ("Unified Mastery", "Seamless, fluent integration", "Polish & mastery"),
    ]
    phases: List[PhaseConfig] = []
    for i, size in enumerate(sizes):
        name, focus, desc = templates[i] if i < len(templates) else (
            f"Phase {i+1}", "Progress integration", "Generic phase"
        )
        phases.append(PhaseConfig(name=name, focus=focus, description=desc, examples=size))
    return phases


def main(argv: Optional[List[str]] = None) -> int:
    args = parse_args(argv)
    try:
        sizes = [int(s) for s in args.phase_sizes.split(",") if s.strip()]
    except ValueError:
        print("[ERROR] Invalid --phase-sizes (must be comma-separated ints)", file=sys.stderr)
        return 2
    phases = build_phase_configs(sizes)
    trainer = HybridTrainer(
        dataset_path=args.dataset,
        model=args.model,
        ollama_url=args.ollama_url,
        seed=args.seed,
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_tokens,
        retries=args.retries,
        retry_backoff=args.retry_backoff,
        stream=args.stream,
        dry_run=args.dry_run,
    )
    try:
        trainer.run(phases)
        return 0
    except KeyboardInterrupt:
        print("[INFO] Keyboard interrupt")
        return 130
    except FileNotFoundError as e:
        print(f"[ERROR] {e}", file=sys.stderr)
        return 1
    except Exception as e:  # noqa: BLE001
        print(f"[ERROR] Unhandled exception: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":  # pragma: no cover
    sys.exit(main())
